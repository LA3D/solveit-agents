{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Understanding how to build Agents using the Solve It approach"]}, {"cell_type": "code", "metadata": {}, "source": ["# !pip install cosette"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["from cosette import *\n", "from fastcore.utils import *\n", "import fastcore.all as fc, re, math, itertools, functools, numpy as np, types, typing, dataclasses, matplotlib.pyplot as plt, collections, regex\n", "from regex import search\n", "from collections import Counter\n", "from collections.abc import Iterable\n", "from pydantic import BaseModel, Field, field_validator\n", "import uuid\n", "from typing import Literal, Union, List, Dict, Any\n", "from toolslm.funccall import get_schema, mk_ns, call_func\n", "from rdflib.plugins.parsers.notation3 import BadSyntax\n", "from rdflib import Graph, URIRef\n", "from pyshacl import validate\n", "import json\n", "import httpx"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Hi solveit! I am Chuck, and I am working with you today! I'm know some Python, experienced with semantic web technologies like OWL,RDF and SPARQL, and I enjoy learning new things.How When writing code for me, please follow fast.ai coding style: avoid comments; place single line if/loop/etc constructs on the same line; use concise variable names where appropriate. Follow the style you will see in my messages, as we work on this dialogue together. Are you ready?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Hi Chuck! Great to meet you! I understand you're experienced with Python and semantic web technologies, and I'll follow the fast.ai coding style you described. \n", "\n", "Before we dive in, could you tell me what specific aspect of building agents you'd like to explore first? Given your background with semantic technologies, we could approach this from several angles - perhaps looking at how to structure agent knowledge, handle tool interactions, or implement reasoning loops.\n", "\n", "What interests you most about agents?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Context Resources\n", "The following resources have been converted to markdown using [web2md](https://web2md.answer.ai/):\n", "- [Anthropics \"Building Effective Agents\"](https://www.anthropic.com/research/building-effective-agents)\n", "- [Topics from Anthropics Prompt Engineering Guide](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n", "- [Cosette is Claudette\u2019s sister, a helper for OpenAI GPT Documentation](https://answerdotai.github.io/cosette/)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Agentic Patterns\n", "Very broadly, Andrew Ng [defines agentic patterns](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/) as:\n", "\n", "- Reflection: The LLM examines its own work to come up with ways to improve it. \n", "- Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.\n", "- Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).\n", "- Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would. This builds on the idea of Roles in Anthropics Prompting Guide.\n", "\n", "The Anthropic Agents [\"Building Effective Agents\"](https://www.anthropic.com/research/building-effective-agents) provides more concrete advice on building agents.\n", "Our Goal is to discover how to build agents using the Solve It approach leveraging Cosette's capabilites to help build the agents."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take an exploratory approach similar to Jeremy Howard's style in building Cosette - starting simple and building understanding through experimentation. We want to use exploratory and literate programming to build agent like systems. We want to leverage Cosette's conversation handling, tool use and tool loops to build on it and provide additional agentic behaviors. \n", "\n", "For the Solve It approach to agents, we should begin by creating a minimal working example that we can iterate on. Would you like to:\n", "\n", "1. Start by creating a basic agent class that we can extend with capabilities, examining each component as we add it\n", "2. Begin with a specific use case (like a research or coding assistant) and evolve our understanding through practical implementation\n", "3. Focus on the interaction patterns first (like the tool loop pattern from Cosette) and build our agent architecture around that\n", "\n", "Cosette already provides the basic conversation and model interaction capabilities.\n", "\n", "Instead of reinventing these components, we should focus on what makes an agent different from a basic chat interface. Looking at the Anthropic Agents documentation, the key differences are:\n", "\n", "1. Agents dynamically direct their own processes\n", "2. They maintain control over how they accomplish tasks\n", "3. They can plan and execute multi-step operations\n", "\n", "Let's explore how we could extend Cosette's capabilities to build these more agentic behaviors starting with:\n", "- Adding planning capabilities to a Chat instance\n", "- Creating a wrapper that manages multi-step task execution"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Planning Architectures using an Orchestrator Pattern that has a TaskLedger\n", "We want to use orchestrator-workers workflow from the anthropic prompt guide, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results. \n", "The key modification is to use Task and Progress Ledgers.\n", "Background:\n", "It has been argued the [\"LLMs Can\u2019t Plan, But Can Help Planning in LLM-Modulo Frameworks\"](https://arxiv.org/html/2402.01817v3).  One crude attempt is the [multi-agent architecture](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/) is where a lead Orchestrator agent is responsible for high-level planning, directing other agents and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan. This is illustrated in the figure above; the Orchestrator work is thus divided into an outer loop where it updates the Task Ledger and an inner loop to update the Progress Ledger.\n", "\n", "The Orchestrator agent implements two loops: an outer loop and an inner loop. The outer loop (lighter background with solid arrows) manages the task ledger (containing facts, guesses, and plan) and the inner loop (darker background with dotted arrows) manages the progress ledger (containing current progress, task assignment to agents).\n", "\n", "The key components we'll need are:\n", "\n", "- A structured way to store facts, guesses, and plans in the TaskLedger\n", "- A way to track progress and agent assignments in the ProgressLedger\n", "- Prompts that implement reflection and planning using Anthropic's best practices"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Concrete example of CoPilot-Agent system\n", "Goal: Create an agent-based assistant that helps scientists make their datasets FAIR compliant\n", "\n", "We're looking at a two-layer architecture where:\n", "\n", "The co-pilot handles human interaction, managing the dialogue flow\n", "The agent handles the autonomous processing, reflection, and planning\n", "Let's start with a simple version of this separation. We could have:\n", "\n", "CoPilot: Handles user interaction using Cosette's Chat\n", "Agent: Uses the TaskLedger and handles the reasoning/planning\n", "The key interaction point would be how they communicate - the co-pilot could pass user inputs to the agent and receive back either:\n", "\n", "Direct responses\n", "Requests for clarification\n", "Status updates on task progress\n", "\n", "\n", "Core Components:\n", "\n", "Dataset CoPilot Interface\n", "\n", "- Handles direct user interaction\n", "- Guides scientists through the process\n", "- Can answer questions on the dataset\n", "- Can use agents that have specific expertise, for example a FAIR Agent\n", "- Asks clarifying questions when needed\n", "\n", "General Dataset Agent\n", "- Can uses metadata to understand data\n", "- Can use URI's as affordances (retreive more information for context)\n", "\n", "Metadata understander/validator\n", "- URL/identifier checker\n", "- Format analyzer\n", "- License checker\n", "\n", "We want to leverage MLCommons Croissant as a FAIR encoding standard."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Example Data from the Titanic Data set.\n", "\n", "The Titanic Data set is available from https://www.kaggle.com/datasets/yasserh/titanic-dataset. It is a classic data set used for machine learning tutorials. It contains information about the passengers on the Titanic, including their age, sex, class, and whether they survived the sinking.\n", "\n", "The croissant spec details are available:\n", "- [Spec Markdown](https://raw.githubusercontent.com/mlcommons/croissant/refs/heads/main/docs/croissant-spec.md)\n", "- [Formal Definitions in RDF Turtle](https://raw.githubusercontent.com/mlcommons/croissant/refs/heads/main/docs/croissant.ttl)"]}, {"cell_type": "code", "metadata": {}, "source": ["tcurl=\"https://storage.googleapis.com/kaggle-data-sets/1818188/2965537/croissant/metadata.json?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1738860546&Signature=sv%2FWTla20YEMWRFThwdkuxxbtk4WKwobqg3eysAizumIHdM6KsxzaDPX2pFp0WA3Fnz3KTQdgxVozOytxl0UCS2hPjDQqbWq2CaWZJ5dWGH%2F1vDoivVvjLefmcPHqKt3%2F1gG5BviIGuT5ZKGBGu2bq9s%2FSB%2Ble4RBKWkO%2BXuocxXql4yqC%2Bl9boYOB8FZFR%2FtPY85kfr7DYP4D5mFo8cg7qe7oFpsqkGKuoKddEWQzq6l%2FQMVzp18s6oVhCItbG4m8FrO65niOrPWrHbrSWQNLQ8wzGL93pcEjK1WAOchxWS%2F4%2FfdlBq4Isf1x57ghz8k9wzDzTvz3sb6KCRIWdTcw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtitanic-dataset-metadata.json\""], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def get_croissant(url, fname='titanic_croissant.json'):\n", "    \"Download Croissant metadata and save to file\"\n", "    try:\n", "        r = httpx.get(url)\n", "        r.raise_for_status()\n", "        with open(fname, 'wb') as f: f.write(r.content)\n", "        return fname\n", "    except Exception as e: print(f'Download failed: {e}')\n", "\n", "fname = get_croissant(tcurl)\n", "if fname:\n", "    with open(fname) as f: data = json.load(f)\n", "    print(f'Successfully loaded {len(data)} metadata fields')"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Successfully loaded 19 metadata fields\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "How do we see which metadata fields were successfully retrieved?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Let's look at the top-level fields in the Croissant metadata in a clean way:\n", "\n", "```python\n", "def show_fields(d, indent=0):\n", "    \"Show fields and their types in a nested dict\"\n", "    for k,v in d.items():\n", "        typ = type(v).__name__\n", "        print(f\"{'  '*indent}{k}: {typ}\")\n", "        if isinstance(v, dict): show_fields(v, indent+1)\n", "        elif isinstance(v, list) and v and isinstance(v[0], dict):\n", "            show_fields(v[0], indent+1)\n", "\n", "show_fields(data)\n", "```\n", "\n", "This will give us a hierarchical view of the metadata structure. We can then dig deeper into any specific fields you're interested in. Would you like to examine any particular field in more detail?"]}, {"cell_type": "code", "metadata": {}, "source": ["def show_fields(d, indent=0):\n", "    \"Show fields and their types in a nested dict\"\n", "    for k,v in d.items():\n", "        typ = type(v).__name__\n", "        print(f\"{'  '*indent}{k}: {typ}\")\n", "        if isinstance(v, dict): show_fields(v, indent+1)\n", "        elif isinstance(v, list) and v and isinstance(v[0], dict):\n", "            show_fields(v[0], indent+1)\n", "\n", "show_fields(data)"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["@context: dict\n", "  @language: str\n", "  @vocab: str\n", "  citeAs: str\n", "  column: str\n", "  conformsTo: str\n", "  cr: str\n", "  data: dict\n", "    @id: str\n", "    @type: str\n", "  dataBiases: str\n", "  dataCollection: str\n", "  dataType: dict\n", "    @id: str\n", "    @type: str\n", "  dct: str\n", "  extract: str\n", "  field: str\n", "  fileProperty: str\n", "  fileObject: str\n", "  fileSet: str\n", "  format: str\n", "  includes: str\n", "  isEnumeration: str\n", "  jsonPath: str\n", "  key: str\n", "  md5: str\n", "  parentField: str\n", "  path: str\n", "  personalSensitiveInformation: str\n", "  recordSet: str\n", "  references: str\n", "  regex: str\n", "  repeated: str\n", "  replace: str\n", "  sc: str\n", "  separator: str\n", "  source: str\n", "  subField: str\n", "  transform: str\n", "  wd: str\n", "alternateName: str\n", "conformsTo: str\n", "license: dict\n", "  @type: str\n", "  name: str\n", "  url: str\n", "distribution: list\n", "  contentUrl: str\n", "  contentSize: str\n", "  md5: str\n", "  encodingFormat: str\n", "  @id: str\n", "  @type: str\n", "  name: str\n", "  description: str\n", "recordSet: list\n", "  field: list\n", "    dataType: list\n", "    source: dict\n", "      fileObject: dict\n", "        @id: str\n", "      extract: dict\n", "        column: str\n", "    @id: str\n", "    @type: str\n", "    name: str\n", "    description: str\n", "  @id: str\n", "  @type: str\n", "  name: str\n", "  description: str\n", "version: int\n", "keywords: list\n", "isAccessibleForFree: bool\n", "includedInDataCatalog: dict\n", "  @type: str\n", "  name: str\n", "  url: str\n", "creator: dict\n", "  @type: str\n", "  name: str\n", "  url: str\n", "  image: str\n", "publisher: dict\n", "  @type: str\n", "  name: str\n", "  url: str\n", "  image: str\n", "thumbnailUrl: str\n", "dateModified: str\n", "datePublished: str\n", "@type: str\n", "name: str\n", "url: str\n", "description: str\n"]}], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["models\n", "models[2]"], "outputs": [{"data": {"text/plain": ["'gpt-4o'"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def get_turtle(url='https://raw.githubusercontent.com/mlcommons/croissant/refs/heads/main/docs/croissant.ttl'):\n", "    \"Get and parse the Croissant vocabulary\"\n", "    g = Graph()\n", "    try:\n", "        r = httpx.get(url)\n", "        r.raise_for_status()\n", "        g.parse(data=r.text, format='turtle')\n", "        return g\n", "    except Exception as e: print(f'Failed to load turtle: {e}')\n", "\n", "g = get_turtle()\n", "if g:\n", "    # Let's see what predicates are defined\n", "    predicates = set(p for _,p,_ in g)\n", "    print(f\"Found {len(predicates)} predicates in schema\")\n", "    # Show a few example predicates\n", "    for p in list(predicates)[:5]: print(p)"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Found 6 predicates in schema\n", "https://schema.org/rangeIncludes\n", "http://www.w3.org/2000/01/rdf-schema#comment\n", "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n", "http://www.w3.org/2000/01/rdf-schema#subClassOf\n", "http://www.w3.org/2000/01/rdf-schema#label\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["I think what we want is a data Agent that can:\n", "- take a question\n", "- decide whether it has enough information available to answer the question\n", "- use the Croissant metadata and Croissant file to help decide if it can answer the question \n", "- decide based on the tools if it can retrieve more information from the URIs present inside the Croissant file\n", "- tell the user that it can answer the question\n", "- answer the question\n", "- provide the supporting evidence for answering the question.\n", "\n", "The simpliset aproach is to use Chat to define an interface as the main conversational co-pilot and then have a dataset agent get registered as a tool. From cosette, the chat should select the dataset agent as a tool and then use it to answer the question. The dataset agent should be able to use the Croissant metadata and Croissant file to answer the question. It should be able to use the URIs present inside the Croissant file to retrieve more information."]}, {"cell_type": "code", "metadata": {}, "source": ["def get_recordset_fields() -> List[Dict[str, Any]]:\n", "    \"\"\"List all available fields in the Titanic dataset.\n", "    Returns a list of basic field information to help identify fields of interest.\n", "    Use this first to find what fields are available.\"\"\"\n", "    recordsets = data.get('recordSet', [])\n", "    if not recordsets: return []\n", "    fields = recordsets[0].get('field', [])\n", "    return [{k:v for k,v in f.items() if k in ('name','description','dataType')} \n", "            for f in fields]\n", "\n", "def get_field_details(field_name:str) -> Dict[str, Any]:\n", "    \"\"\"Get complete Croissant metadata for a specific field.\n", "    Args:\n", "        field_name: Exact name of field to lookup\n", "    Returns:\n", "        Full Croissant field definition including source and structure information.\n", "    Use this after finding a field of interest to get its complete definition.\"\"\"\n", "    fields = data.get('recordSet', [])[0].get('field', [])\n", "    field = next((f for f in fields if f.get('name')==field_name), {})\n", "    return {'field': field} if field else {'error': f'Field {field_name} not found'}"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def dataset_explorer(question:str):\n", "    \"Enhanced interface for exploring the Titanic dataset\"\n", "    def get_metadata_section(section:str) -> Dict:\n", "        \"Get a section like recordSet, distribution from metadata\"\n", "        result = data.get(section, {})\n", "        return {'section': section, 'content': result}\n", "    \n", "    def get_field_info(field_name:str) -> Dict:\n", "        \"Get field info including schema validation\"\n", "        basic = get_recordset_fields()\n", "        details = get_field_details(field_name)\n", "        \n", "        # Handle nested lists in field data\n", "        field_data = details.get('field', {})\n", "        if isinstance(field_data, list): \n", "            field_data = field_data[0]\n", "            \n", "        # Handle dataType list and get the full URI\n", "        data_type = field_data.get('dataType', [])\n", "        if isinstance(data_type, list) and data_type:\n", "            data_type = data_type[0]\n", "        \n", "        return {\n", "            'basic': next((f for f in basic if f['name']==field_name), {}),\n", "            'details': field_data,\n", "            'schema_type': schema.get(data_type, {})  # data_type is now the string directly\n", "        }\n", "    \n", "    chat = Chat(models[2], tools=[\n", "        get_metadata_section,\n", "        get_field_info\n", "    ],\n", "    sp=\"\"\"You are a dataset explorer that helps understand datasets through their Croissant metadata.\n", "    For each question:\n", "    1. State which tools you will use and why\n", "    2. Show the results from each tool call explicitly\n", "    3. Analyze how the findings align with Croissant specifications\n", "    4. Provide a clear summary answer\"\"\")\n", "    \n", "    return chat.toolloop(question, trace_func=print)\n", "\n", "# Let's try again\n", "dataset_explorer(\"Tell me about the Age field and how it's structured according to Croissant specifications.\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["ChatCompletion(id='chatcmpl-Awwvie4ZEZUAM4kdOBb9tRucSPX6R', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_MS7m0qqZ9hxkAaUAXpI0VG0J', function=Function(arguments='{\"field_name\":\"Age\"}', name='get_field_info'), type='function')]))], created=1738612914, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 174; Out: 17; Total: 191)\n", "ChatCompletion(id='chatcmpl-Awwvk2NJ32itlK3AywpJzotbB0Hde', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To understand the \"Age\" field according to Croissant specifications, I used the `functions.get_field_info` tool. Here\\'s the detailed information about the \"Age\" field:\\n\\n1. **Basic Information:**\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n   - **Data Type:** Float (`sc:Float`)\\n\\n2. **Detailed Information:**\\n   - **Data Type:** Float (`sc:Float`)\\n   - **Source:** \\n     - **File Object ID:** Titanic-Dataset.csv_fileobject\\n     - **Column Extracted From:** Age\\n   - **Field ID:** Titanic-Dataset.csv/Age\\n   - **Field Type:** `cr:Field`\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n\\n3. **Schema Type:** No specific schema type provided.\\n\\n### Analysis and Alignment with Croissant Specifications\\n\\n- The Croissant specification provides a clear architectural framework for metadata annotations. In alignment with these specifications:\\n  - The \"Age\" field has been distinctly identified with a `name` and a `description`, both of which are concise and informative.\\n  - The `dataType` identified as `sc:Float` indicates that the \"Age\" field is expressed numerically as a floating-point number, which is essential for calculations that might involve non-integer ages or missing values represented by floats.\\n  - The source information details both the file object ID and the specific column from which this field is extracted, ensuring traceability and reusability in data lineage.\\n\\n### Summary\\n\\nThe \"Age\" field in this dataset is a numerical field structured as a floating-point value, and it represents ages in years. According to Croissant specifications, relevant metadata such as field description, data type, and provenance are well-documented, ensuring clarity in data structure and usage.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738612916, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 307; Out: 375; Total: 682)\n"]}, {"data": {"text/plain": ["ChatCompletion(id='chatcmpl-Awwvk2NJ32itlK3AywpJzotbB0Hde', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To understand the \"Age\" field according to Croissant specifications, I used the `functions.get_field_info` tool. Here\\'s the detailed information about the \"Age\" field:\\n\\n1. **Basic Information:**\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n   - **Data Type:** Float (`sc:Float`)\\n\\n2. **Detailed Information:**\\n   - **Data Type:** Float (`sc:Float`)\\n   - **Source:** \\n     - **File Object ID:** Titanic-Dataset.csv_fileobject\\n     - **Column Extracted From:** Age\\n   - **Field ID:** Titanic-Dataset.csv/Age\\n   - **Field Type:** `cr:Field`\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n\\n3. **Schema Type:** No specific schema type provided.\\n\\n### Analysis and Alignment with Croissant Specifications\\n\\n- The Croissant specification provides a clear architectural framework for metadata annotations. In alignment with these specifications:\\n  - The \"Age\" field has been distinctly identified with a `name` and a `description`, both of which are concise and informative.\\n  - The `dataType` identified as `sc:Float` indicates that the \"Age\" field is expressed numerically as a floating-point number, which is essential for calculations that might involve non-integer ages or missing values represented by floats.\\n  - The source information details both the file object ID and the specific column from which this field is extracted, ensuring traceability and reusability in data lineage.\\n\\n### Summary\\n\\nThe \"Age\" field in this dataset is a numerical field structured as a floating-point value, and it represents ages in years. According to Croissant specifications, relevant metadata such as field description, data type, and provenance are well-documented, ensuring clarity in data structure and usage.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738612916, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 307; Out: 375; Total: 682)"], "text/markdown": ["To understand the \"Age\" field according to Croissant specifications, I used the `functions.get_field_info` tool. Here's the detailed information about the \"Age\" field:\n", "\n", "1. **Basic Information:**\n", "   - **Name:** Age\n", "   - **Description:** Age in Years\n", "   - **Data Type:** Float (`sc:Float`)\n", "\n", "2. **Detailed Information:**\n", "   - **Data Type:** Float (`sc:Float`)\n", "   - **Source:** \n", "     - **File Object ID:** Titanic-Dataset.csv_fileobject\n", "     - **Column Extracted From:** Age\n", "   - **Field ID:** Titanic-Dataset.csv/Age\n", "   - **Field Type:** `cr:Field`\n", "   - **Name:** Age\n", "   - **Description:** Age in Years\n", "\n", "3. **Schema Type:** No specific schema type provided.\n", "\n", "### Analysis and Alignment with Croissant Specifications\n", "\n", "- The Croissant specification provides a clear architectural framework for metadata annotations. In alignment with these specifications:\n", "  - The \"Age\" field has been distinctly identified with a `name` and a `description`, both of which are concise and informative.\n", "  - The `dataType` identified as `sc:Float` indicates that the \"Age\" field is expressed numerically as a floating-point number, which is essential for calculations that might involve non-integer ages or missing values represented by floats.\n", "  - The source information details both the file object ID and the specific column from which this field is extracted, ensuring traceability and reusability in data lineage.\n", "\n", "### Summary\n", "\n", "The \"Age\" field in this dataset is a numerical field structured as a floating-point value, and it represents ages in years. According to Croissant specifications, relevant metadata such as field description, data type, and provenance are well-documented, ensuring clarity in data structure and usage.\n", "\n", "<details>\n", "\n", "- id: chatcmpl-Awwvk2NJ32itlK3AywpJzotbB0Hde\n", "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To understand the \"Age\" field according to Croissant specifications, I used the `functions.get_field_info` tool. Here\\'s the detailed information about the \"Age\" field:\\n\\n1. **Basic Information:**\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n   - **Data Type:** Float (`sc:Float`)\\n\\n2. **Detailed Information:**\\n   - **Data Type:** Float (`sc:Float`)\\n   - **Source:** \\n     - **File Object ID:** Titanic-Dataset.csv_fileobject\\n     - **Column Extracted From:** Age\\n   - **Field ID:** Titanic-Dataset.csv/Age\\n   - **Field Type:** `cr:Field`\\n   - **Name:** Age\\n   - **Description:** Age in Years\\n\\n3. **Schema Type:** No specific schema type provided.\\n\\n### Analysis and Alignment with Croissant Specifications\\n\\n- The Croissant specification provides a clear architectural framework for metadata annotations. In alignment with these specifications:\\n  - The \"Age\" field has been distinctly identified with a `name` and a `description`, both of which are concise and informative.\\n  - The `dataType` identified as `sc:Float` indicates that the \"Age\" field is expressed numerically as a floating-point number, which is essential for calculations that might involve non-integer ages or missing values represented by floats.\\n  - The source information details both the file object ID and the specific column from which this field is extracted, ensuring traceability and reusability in data lineage.\\n\\n### Summary\\n\\nThe \"Age\" field in this dataset is a numerical field structured as a floating-point value, and it represents ages in years. According to Croissant specifications, relevant metadata such as field description, data type, and provenance are well-documented, ensuring clarity in data structure and usage.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n", "- created: 1738612916\n", "- model: gpt-4o-2024-08-06\n", "- object: chat.completion\n", "- service_tier: default\n", "- system_fingerprint: fp_4691090a87\n", "- usage: CompletionUsage(completion_tokens=375, prompt_tokens=307, total_tokens=682, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n", "\n", "</details>"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Using data types from other vocabularies\n", "\n", "Croissant datasets can use data types from other vocabularies, such as Wikidata. These may be supported by the tools consuming the data, but don\u2019t need to. For example:\n", "\n", "<table>\n", "  <thead>\n", "    <th>dataType</th>\n", "    <th>Usage</th>\n", "  </thead>\n", "  <tr>\n", "    <td>\n", "      <a href=\"https://www.wikidata.org/wiki/Q48277\">wd:Q48277</a><br>\n", "      (gender)\n", "    </td>\n", "    <td>Describes a Field or a RecordSet whose values are indicative of someone\u2019s gender. This could be used for instance by RAI frameworks and tools to flag possible biases in the data. Values for this RecordSet can be associated with specific gender URLs (eg: <a href=\"https://www.wikidata.org/wiki/Q6581097\">wd:Q6581097</a>, <a href=\"https://www.wikidata.org/wiki/Q6581072\">wd:Q6581072</a>, etc.). Refer to the \"Typed RecordSets > Enumerations\" section for an example.</td>\n", "  </tr>\n", "</table>\n", "\n", "In the following example, `color_sample` is a field containing an image, but with no associated machine-readable semantics.\n", "\n", "```json\n", "{\n", "  \"@id\": \"images/color_sample\",\n", "  \"@type\": \"cr:Field\",\n", "  \"dataType\": \"sc:ImageObject\"\n", "}\n", "```\n", "\n", "In the following example, the `url` field is expected to be a URL, whose semantic type is [City](https://www.wikidata.org/wiki/Q515), so one will expect values of this field to be URLs referring to cities (e.g.: \"<https://www.wikidata.org/wiki/Q90>\").\n", "\n", "```json\n", "{\n", "  \"@id\": \"cities/url\",\n", "  \"@type\": \"cr:Field\",\n", "  \"dataType\": [\"https://schema.org/URL\", \"https://www.wikidata.org/wiki/Q515\"]\n", "}\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "This doesn't seem to be very agentic. We're actually hard-coding a whole bunch of stuff that the agent should basically think through and decide that it needs to answer a particular question. We are also not using any of the deeper semantics that we get from JSON-LD. There are namespace definitions like sc:,dct:,wd:. Particularly the wd: namespace corresponds to wikidata."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You make an excellent point! We're currently doing too much hand-holding rather than letting the agent reason about and use the semantic capabilities available. Let's make it more agentic by:\n", "\n", "1. First, let's create a more semantic-aware base that can understand and use JSON-LD contexts:\n", "\n", "```python\n", "class SemanticContext:\n", "    \"Handle JSON-LD context and namespace resolution\"\n", "    def __init__(self, context):\n", "        self.ctx = context['@context']\n", "        self.namespaces = {k:v for k,v in self.ctx.items() \n", "                          if isinstance(v,str) and ':' not in k}\n", "    \n", "    def resolve_uri(self, term):\n", "        \"Resolve a compact URI (CURIE) to its full URI\"\n", "        if ':' in term:\n", "            prefix,local = term.split(':')\n", "            if prefix in self.namespaces:\n", "                return f\"{self.namespaces[prefix]}{local}\"\n", "        return term\n", "\n", "    def get_linked_data(self, uri):\n", "        \"Fetch linked data from a URI (e.g. from Wikidata)\"\n", "        if uri.startswith('http://www.wikidata.org'):\n", "            try: return httpx.get(f\"{uri.replace('http:', 'https:')}.json\").json()\n", "            except: return None\n", "        return None\n", "```\n", "\n", "2. Now let's create a more agentic dataset explorer that can reason about what it needs:\n", "\n", "```python\n", "class DatasetAgent:\n", "    def __init__(self, metadata, model=models[2]):\n", "        self.metadata = metadata\n", "        self.semantic = SemanticContext(metadata)\n", "        self.chat = Chat(model, tools=[\n", "            self.explore_metadata,\n", "            self.get_semantic_info,\n", "            self.check_field_semantics\n", "        ])\n", "    \n", "    def explore_metadata(self, path:str) -> Dict:\n", "        \"Explore any path in the metadata\"\n", "        parts = path.split('/')\n", "        curr = self.metadata\n", "        for p in parts:\n", "            if isinstance(curr, list): curr = curr[0]\n", "            curr = curr.get(p,{})\n", "        return curr\n", "    \n", "    def get_semantic_info(self, term:str) -> Dict:\n", "        \"Get semantic information about a term\"\n", "        uri = self.semantic.resolve_uri(term)\n", "        return {\n", "            'uri': uri,\n", "            'linked_data': self.semantic.get_linked_data(uri)\n", "        }\n", "        \n", "    def check_field_semantics(self, field_name:str) -> Dict:\n", "        \"Check semantic annotations for a field\"\n", "        field = next((f for rs in self.metadata.get('recordSet',[])\n", "                     for f in rs.get('field',[])\n", "                     if f.get('name')==field_name), {})\n", "        return {\n", "            'datatypes': [self.get_semantic_info(dt) \n", "                         for dt in field.get('dataType',[])],\n", "            'annotations': field.get('annotations',[])\n", "        }\n", "    \n", "    def answer(self, question:str):\n", "        \"Answer questions about the dataset\"\n", "        return self.chat.toolloop(\n", "            f\"\"\"Question about dataset: {question}\n", "            Available tools:\n", "            - explore_metadata: Explore any path in the metadata\n", "            - get_semantic_info: Get semantic information about terms\n", "            - check_field_semantics: Check semantic annotations for fields\n", "            \n", "            Think through:\n", "            1. What semantic information might help answer this?\n", "            2. What parts of the metadata structure need to be explored?\n", "            3. How can you combine the information to give a complete answer?\n", "            \"\"\")\n", "\n", "# Create and use the agent\n", "agent = DatasetAgent(data)\n", "```\n", "\n", "Would you like to test this more semantic-aware agent with a question? For example, we could ask it about age-related fields and their semantic connections to Wikidata concepts, or explore how fields are semantically typed?\n", "\n", "This version:\n", "1. Understands and can resolve JSON-LD contexts\n", "2. Can fetch linked data from Wikidata when referenced\n", "3. Reasons about what information it needs rather than following a fixed script\n", "4. Can explore the metadata structure dynamically\n", "5. Maintains semantic context across interactions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Ok. We still want to use the JH approach."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Ah yes, let's follow Jeremy Howard's iterative approach:\n", "\n", "1. Start with the absolute minimum that could work\n", "2. Test it\n", "3. See what's actually needed\n", "4. Iterate with small, focused improvements\n", "\n", "Here's a simpler starting point:\n", "\n", "```python\n", "class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): self.data = data\n", "    \n", "    @property\n", "    def context(self): return self.data.get('@context',{})\n", "    \n", "    def resolve(self, curie):\n", "        \"Resolve a CURIE (e.g. 'sc:Text' or 'wd:Q515') to full URI\"\n", "        if ':' not in curie: return curie\n", "        prefix,local = curie.split(':')\n", "        base = self.context.get(prefix)\n", "        return f\"{base}{local}\" if base else curie\n", "    \n", "    def __call__(self, q): \n", "        \"Basic question answering about the dataset\"\n", "        print(f'Resolving semantic terms in: {q}')\n", "        return {k:self.resolve(v) for k,v in self.context.items() \n", "                if isinstance(v,str) and ':' in v}\n", "\n", "agent = DatasetAgent(data)\n", "agent(\"What semantic namespaces are used in this dataset?\")\n", "```\n", "\n", "This gives us a minimal foundation to explore:\n", "1. How the agent actually gets used\n", "2. What capabilities we really need\n", "3. Where to focus our attention\n", "\n", "Shall we try this simpler version and see what we actually need to add based on usage?"]}, {"cell_type": "code", "metadata": {}, "source": ["class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): self.data = data\n", "    \n", "    @property\n", "    def context(self): return self.data.get('@context',{})\n", "    \n", "    def resolve(self, curie):\n", "        \"Resolve a CURIE (e.g. 'sc:Text' or 'wd:Q515') to full URI\"\n", "        if ':' not in curie: return curie\n", "        prefix,local = curie.split(':')\n", "        base = self.context.get(prefix)\n", "        return f\"{base}{local}\" if base else curie\n", "    \n", "    def __call__(self, q): \n", "        \"Basic question answering about the dataset\"\n", "        print(f'Resolving semantic terms in: {q}')\n", "        return {k:self.resolve(v) for k,v in self.context.items() \n", "                if isinstance(v,str) and ':' in v}\n", "\n", "agent = DatasetAgent(data)\n", "agent(\"What semantic namespaces are used in this dataset?\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Resolving semantic terms in: What semantic namespaces are used in this dataset?\n"]}, {"data": {"text/plain": ["{'@vocab': 'https://schema.org/',\n", " 'citeAs': 'http://mlcommons.org/croissant/citeAs',\n", " 'column': 'http://mlcommons.org/croissant/column',\n", " 'conformsTo': 'http://purl.org/dc/terms/conformsTo',\n", " 'cr': 'http://mlcommons.org/croissant/',\n", " 'dataBiases': 'http://mlcommons.org/croissant/dataBiases',\n", " 'dataCollection': 'http://mlcommons.org/croissant/dataCollection',\n", " 'dct': 'http://purl.org/dc/terms/',\n", " 'extract': 'http://mlcommons.org/croissant/extract',\n", " 'field': 'http://mlcommons.org/croissant/field',\n", " 'fileProperty': 'http://mlcommons.org/croissant/fileProperty',\n", " 'fileObject': 'http://mlcommons.org/croissant/fileObject',\n", " 'fileSet': 'http://mlcommons.org/croissant/fileSet',\n", " 'format': 'http://mlcommons.org/croissant/format',\n", " 'includes': 'http://mlcommons.org/croissant/includes',\n", " 'isEnumeration': 'http://mlcommons.org/croissant/isEnumeration',\n", " 'jsonPath': 'http://mlcommons.org/croissant/jsonPath',\n", " 'key': 'http://mlcommons.org/croissant/key',\n", " 'md5': 'http://mlcommons.org/croissant/md5',\n", " 'parentField': 'http://mlcommons.org/croissant/parentField',\n", " 'path': 'http://mlcommons.org/croissant/path',\n", " 'personalSensitiveInformation': 'http://mlcommons.org/croissant/personalSensitiveInformation',\n", " 'recordSet': 'http://mlcommons.org/croissant/recordSet',\n", " 'references': 'http://mlcommons.org/croissant/references',\n", " 'regex': 'http://mlcommons.org/croissant/regex',\n", " 'repeated': 'http://mlcommons.org/croissant/repeated',\n", " 'replace': 'http://mlcommons.org/croissant/replace',\n", " 'sc': 'https://schema.org/',\n", " 'separator': 'http://mlcommons.org/croissant/separator',\n", " 'source': 'http://mlcommons.org/croissant/source',\n", " 'subField': 'http://mlcommons.org/croissant/subField',\n", " 'transform': 'http://mlcommons.org/croissant/transform',\n", " 'wd': 'https://www.wikidata.org/wiki/'}"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Ok. How does this help us?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "This initial version shows us a few important things:\n", "\n", "1. We can now see all the namespaces being used:\n", "   - `sc:` (schema.org) for basic data types\n", "   - `cr:` (mlcommons.org/croissant) for dataset-specific concepts\n", "   - `wd:` (wikidata.org) for semantic concepts\n", "   - `dct:` (purl.org/dc/terms) for metadata terms\n", "\n", "2. The next minimal step would be to explore how these namespaces are actually used in our dataset. Let's add that:\n", "\n", "```python\n", "class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): self.data = data\n", "    \n", "    @property\n", "    def context(self): return self.data.get('@context',{})\n", "    \n", "    def resolve(self, curie): \n", "        \"Resolve a CURIE (e.g. 'sc:Text' or 'wd:Q515') to full URI\"\n", "        if ':' not in curie: return curie\n", "        prefix,local = curie.split(':')\n", "        base = self.context.get(prefix)\n", "        return f\"{base}{local}\" if base else curie\n", "    \n", "    def find_terms(self, prefix):\n", "        \"Find all uses of a namespace prefix in the dataset\"\n", "        def _search(d):\n", "            if isinstance(d,dict):\n", "                for k,v in d.items():\n", "                    if isinstance(v,str) and v.startswith(f'{prefix}:'): yield k,v\n", "                    yield from _search(v)\n", "            elif isinstance(d,list):\n", "                for v in d: yield from _search(v)\n", "        return list(_search(self.data))\n", "\n", "agent = DatasetAgent(data)\n", "print(\"Schema.org types used:\")\n", "print(agent.find_terms('sc'))\n", "```\n", "\n", "This lets us explore how semantic types are actually being used in the dataset. Would you like to try this and see what we learn about how the dataset uses semantic types?\n", "\n", "The key here is we're building up our understanding incrementally:\n", "1. First we found what vocabularies are available\n", "2. Now we can see how they're actually used\n", "3. Next we might want to explore specific fields and their semantic meanings"]}, {"cell_type": "code", "metadata": {}, "source": ["class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): self.data = data\n", "    \n", "    @property\n", "    def context(self): return self.data.get('@context',{})\n", "    \n", "    def resolve(self, curie): \n", "        \"Resolve a CURIE (e.g. 'sc:Text' or 'wd:Q515') to full URI\"\n", "        if ':' not in curie: return curie\n", "        prefix,local = curie.split(':')\n", "        base = self.context.get(prefix)\n", "        return f\"{base}{local}\" if base else curie\n", "    \n", "    def find_terms(self, prefix):\n", "        \"Find all uses of a namespace prefix in the dataset\"\n", "        def _search(d):\n", "            if isinstance(d,dict):\n", "                for k,v in d.items():\n", "                    if isinstance(v,str) and v.startswith(f'{prefix}:'): yield k,v\n", "                    yield from _search(v)\n", "            elif isinstance(d,list):\n", "                for v in d: yield from _search(v)\n", "        return list(_search(self.data))\n", "\n", "agent = DatasetAgent(data)\n", "print(\"Schema.org types used:\")\n", "print(agent.find_terms('sc'))"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Schema.org types used:\n", "[('@type', 'sc:CreativeWork'), ('@type', 'sc:DataCatalog'), ('@type', 'sc:Person'), ('@type', 'sc:Organization'), ('@type', 'sc:Dataset')]\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Ok, now what?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Now we can see that our dataset uses several schema.org types (`sc:`) for basic metadata, but we probably want to know more about:\n", "\n", "1. The actual data fields and their types\n", "2. Any Wikidata concepts (`wd:`) that might give us semantic meaning\n", "\n", "Let's extend our minimal agent to explore the `recordSet` where the actual field definitions live:\n", "\n", "```python\n", "class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): \n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "    \n", "    def field_types(self, field_name):\n", "        \"Get the semantic types for a field\"\n", "        field = self.fields.get(field_name,{})\n", "        return [t for t in field.get('dataType',[]) if ':' in t]\n", "    \n", "    def __call__(self, field_name):\n", "        \"Show semantic information about a field\"\n", "        return {'name': field_name,\n", "                'types': self.field_types(field_name),\n", "                'desc': self.fields.get(field_name,{}).get('description')}\n", "\n", "agent = DatasetAgent(data)\n", "print(\"Fields available:\", ', '.join(agent.fields))\n", "print(\"\\nLet's look at Sex field:\")\n", "print(agent('Sex'))\n", "```\n", "\n", "This should show us what semantic types are being used to describe the actual data. Shall we run this and see what we learn about how the Titanic dataset is semantically annotated?"]}, {"cell_type": "code", "metadata": {}, "source": ["class DatasetAgent:\n", "    \"Minimal agent that can explore a dataset's semantic structure\"\n", "    def __init__(self, data): \n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "    \n", "    def field_types(self, field_name):\n", "        \"Get the semantic types for a field\"\n", "        field = self.fields.get(field_name,{})\n", "        return [t for t in field.get('dataType',[]) if ':' in t]\n", "    \n", "    def __call__(self, field_name):\n", "        \"Show semantic information about a field\"\n", "        return {'name': field_name,\n", "                'types': self.field_types(field_name),\n", "                'desc': self.fields.get(field_name,{}).get('description')}\n", "\n", "agent = DatasetAgent(data)\n", "print(\"Fields available:\", ', '.join(agent.fields))\n", "print(\"\\nLet's look at Sex field:\")\n", "print(agent('Sex'))"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Fields available: PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\n", "\n", "Let's look at Sex field:\n", "{'name': 'Sex', 'types': ['sc:Text'], 'desc': 'Gender'}\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Did this work?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Yes, it worked! But it revealed something interesting - the Titanic dataset isn't using the rich semantic capabilities it could be. Let me explain:\n", "\n", "1. We can see all fields in the dataset: PassengerId, Survived, Pclass, Name, Sex, Age, etc.\n", "\n", "2. Looking at the 'Sex' field specifically, we found:\n", "   - It's only typed as `sc:Text` (basic schema.org text type)\n", "   - Description is simply \"Gender\"\n", "   - It's NOT using `wd:Q48277` (Wikidata's gender concept) which would be more semantically meaningful\n", "\n", "This suggests our next step might be to:\n", "\n", "1. Either validate that other fields have richer semantic annotations, or\n", "2. Show how we could enhance this dataset with better semantic typing\n", "\n", "Want to:\n", "- Check another field to see if any have richer semantic typing? or\n", "- Add a method to suggest semantic enhancements (e.g. suggesting Wikidata concepts that could be used)?\n", "\n", "The key insight is that while our agent works, it's showing us that the dataset isn't using the full power of Croissant's semantic capabilities. Should we explore how to improve that?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I think this is a really interesting example of how an LLM can explore a dataset's semantic structure. However, it was done through succesive manual prompts and code definitions to come to this understanding. How do we build an agentic system to do what you just did?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Ah, now we're getting to the heart of it! Instead of us manually exploring and discovering these insights, we want an agent that can autonomously:\n", "\n", "1. Explore the semantic structure\n", "2. Identify gaps and opportunities\n", "3. Suggest improvements\n", "\n", "Let's start with a minimal agentic version using Cosette's tool loop pattern:\n", "\n", "```python\n", "class SemanticExplorer:\n", "    \"Agent that autonomously explores dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics\n", "        ])\n", "    \n", "    def list_fields(self) -> Dict[str,str]:\n", "        \"List all fields and their basic types\"\n", "        return {k: f.get('dataType',[]) for k,f in self.fields.items()}\n", "    \n", "    def check_field(self, name:str) -> Dict:\n", "        \"Get detailed information about a field\"\n", "        return self.fields.get(name, {})\n", "    \n", "    def suggest_semantics(self, field:str) -> Dict:\n", "        \"Suggest semantic enhancements for a field\"\n", "        desc = self.fields[field].get('description','')\n", "        types = self.fields[field].get('dataType',[])\n", "        return {\n", "            'current': {'desc': desc, 'types': types},\n", "            'field': field,\n", "        }\n", "    \n", "    def explore(self):\n", "        \"Autonomously explore the dataset's semantic structure\"\n", "        return self.chat.toolloop(\"\"\"Analyze this dataset's semantic structure.\n", "            1. First check what fields are available\n", "            2. For each field, evaluate its semantic typing\n", "            3. Identify opportunities for semantic enhancement\n", "            4. Provide a summary of findings and recommendations\n", "            \n", "            Focus particularly on fields that could benefit from \n", "            Wikidata concept links or richer schema.org types.\"\"\")\n", "\n", "explorer = SemanticExplorer(data)\n", "explorer.explore()\n", "```\n", "\n", "This agent should:\n", "1. Use Cosette's tool loop to maintain a conversation\n", "2. Autonomously explore the structure rather than waiting for our prompts\n", "3. Build up its understanding through successive tool calls\n", "4. Provide recommendations for semantic enhancements\n", "\n", "Want to try this and see how it performs its semantic analysis? We can then iterate on what additional capabilities it needs based on its actual behavior."]}, {"cell_type": "code", "metadata": {}, "source": ["class SemanticExplorer:\n", "    \"Agent that autonomously explores dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics\n", "        ])\n", "    \n", "    def list_fields(self) -> Dict[str,str]:\n", "        \"List all fields and their basic types\"\n", "        return {k: f.get('dataType',[]) for k,f in self.fields.items()}\n", "    \n", "    def check_field(self, name:str) -> Dict:\n", "        \"Get detailed information about a field\"\n", "        return self.fields.get(name, {})\n", "    \n", "    def suggest_semantics(self, field:str) -> Dict:\n", "        \"Suggest semantic enhancements for a field\"\n", "        desc = self.fields[field].get('description','')\n", "        types = self.fields[field].get('dataType',[])\n", "        return {\n", "            'current': {'desc': desc, 'types': types},\n", "            'field': field,\n", "        }\n", "    \n", "    def explore(self):\n", "        \"Autonomously explore the dataset's semantic structure\"\n", "        return self.chat.toolloop(\"\"\"Analyze this dataset's semantic structure.\n", "            1. First check what fields are available\n", "            2. For each field, evaluate its semantic typing\n", "            3. Identify opportunities for semantic enhancement\n", "            4. Provide a summary of findings and recommendations\n", "            \n", "            Focus particularly on fields that could benefit from \n", "            Wikidata concept links or richer schema.org types.\"\"\")\n", "\n", "explorer = SemanticExplorer(data)\n", "explorer.explore()"], "outputs": [{"data": {"text/plain": ["ChatCompletion(id='chatcmpl-Awy6ypd3psKnEiwiffUPyT31efCwC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"### Summary of Findings and Recommendations\\n\\n#### Dataset Analysis\\nThe dataset in question includes the following fields:\\n\\n1. **PassengerId**: A unique identifier for each passenger, currently typed as text (`sc:Text`).\\n2. **Survived**: An integer (`sc:Integer`) indicating whether a passenger survived (1) or not (0).\\n3. **Pclass**: An integer (`sc:Integer`) representing the passenger's ticket class (1st, 2nd, or 3rd class).\\n4. **Name**: Textual representation of the passenger's full name (`sc:Text`).\\n5. **Sex**: Indicates the gender of the passenger, stored as text (`sc:Text`).\\n6. **Age**: A float (`sc:Float`) indicating the age of the passenger in years.\\n7. **SibSp**: An integer (`sc:Integer`) denoting the number of siblings/spouses aboard the Titanic.\\n8. **Parch**: An integer (`sc:Integer`) for the number of parents/children aboard.\\n9. **Ticket**: The ticket number, represented as text (`sc:Text`).\\n10. **Fare**: A float (`sc:Float`) detailing the passenger's fare.\\n11. **Cabin**: The cabin number, stored as text (`sc:Text`).\\n12. **Embarked**: Textual data (`sc:Text`) encoding the port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\\n\\n#### Recommendations for Semantic Enhancement\\n\\n1. **PassengerId**: This field is currently internally significant but lacks external semantic utility. No external link suggestions.\\n   \\n2. **Survived and Pclass**: These categorical numerical fields could benefit from richer schema descriptors like `schema:EventAttendanceModeEnumeration` or `schema:Seat` for more explicit class data.\\n\\n3. **Name and Sex**: \\n   - **Name** could be enhanced with `schema:Person`.\\n   - **Sex** can potentially be linked to Wikidata concepts related to gender identity.\\n\\n4. **Age**: Already a float, could leverage schema fields like `schema:age` for integration into demographic studies.\\n\\n5. **SibSp and Parch**: These fields could be semantically enhanced using `schema:QuantitativeValue` to express familial relationships in more detail.\\n\\n6. **Ticket and Fare**: \\n   - **Ticket** numbers could link to transportation schema types though this field is typically not semantically enriched.\\n   - **Fare** can use `schema:PriceSpecification`.\\n\\n7. **Cabin**: Potentially enrich using `schema:LodgingUnitType` for a conceptual link to accommodations.\\n\\n8. **Embarked**: This field could be enhanced with `schema:Place` for links to known ports on Wikidata or schema.org locations.\\n\\n#### Overall Recommendations:\\nFields like `Name`, `Sex`, `Age`, and `Embarked` offer the most immediate opportunities for enrichment via established schemas and Wikidata links, aiding both data integration and enhancing contextual understanding in related datasets or applications.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738617456, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 2113; Out: 641; Total: 2754)"], "text/markdown": ["### Summary of Findings and Recommendations\n", "\n", "#### Dataset Analysis\n", "The dataset in question includes the following fields:\n", "\n", "1. **PassengerId**: A unique identifier for each passenger, currently typed as text (`sc:Text`).\n", "2. **Survived**: An integer (`sc:Integer`) indicating whether a passenger survived (1) or not (0).\n", "3. **Pclass**: An integer (`sc:Integer`) representing the passenger's ticket class (1st, 2nd, or 3rd class).\n", "4. **Name**: Textual representation of the passenger's full name (`sc:Text`).\n", "5. **Sex**: Indicates the gender of the passenger, stored as text (`sc:Text`).\n", "6. **Age**: A float (`sc:Float`) indicating the age of the passenger in years.\n", "7. **SibSp**: An integer (`sc:Integer`) denoting the number of siblings/spouses aboard the Titanic.\n", "8. **Parch**: An integer (`sc:Integer`) for the number of parents/children aboard.\n", "9. **Ticket**: The ticket number, represented as text (`sc:Text`).\n", "10. **Fare**: A float (`sc:Float`) detailing the passenger's fare.\n", "11. **Cabin**: The cabin number, stored as text (`sc:Text`).\n", "12. **Embarked**: Textual data (`sc:Text`) encoding the port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n", "\n", "#### Recommendations for Semantic Enhancement\n", "\n", "1. **PassengerId**: This field is currently internally significant but lacks external semantic utility. No external link suggestions.\n", "   \n", "2. **Survived and Pclass**: These categorical numerical fields could benefit from richer schema descriptors like `schema:EventAttendanceModeEnumeration` or `schema:Seat` for more explicit class data.\n", "\n", "3. **Name and Sex**: \n", "   - **Name** could be enhanced with `schema:Person`.\n", "   - **Sex** can potentially be linked to Wikidata concepts related to gender identity.\n", "\n", "4. **Age**: Already a float, could leverage schema fields like `schema:age` for integration into demographic studies.\n", "\n", "5. **SibSp and Parch**: These fields could be semantically enhanced using `schema:QuantitativeValue` to express familial relationships in more detail.\n", "\n", "6. **Ticket and Fare**: \n", "   - **Ticket** numbers could link to transportation schema types though this field is typically not semantically enriched.\n", "   - **Fare** can use `schema:PriceSpecification`.\n", "\n", "7. **Cabin**: Potentially enrich using `schema:LodgingUnitType` for a conceptual link to accommodations.\n", "\n", "8. **Embarked**: This field could be enhanced with `schema:Place` for links to known ports on Wikidata or schema.org locations.\n", "\n", "#### Overall Recommendations:\n", "Fields like `Name`, `Sex`, `Age`, and `Embarked` offer the most immediate opportunities for enrichment via established schemas and Wikidata links, aiding both data integration and enhancing contextual understanding in related datasets or applications.\n", "\n", "<details>\n", "\n", "- id: chatcmpl-Awy6ypd3psKnEiwiffUPyT31efCwC\n", "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"### Summary of Findings and Recommendations\\n\\n#### Dataset Analysis\\nThe dataset in question includes the following fields:\\n\\n1. **PassengerId**: A unique identifier for each passenger, currently typed as text (`sc:Text`).\\n2. **Survived**: An integer (`sc:Integer`) indicating whether a passenger survived (1) or not (0).\\n3. **Pclass**: An integer (`sc:Integer`) representing the passenger's ticket class (1st, 2nd, or 3rd class).\\n4. **Name**: Textual representation of the passenger's full name (`sc:Text`).\\n5. **Sex**: Indicates the gender of the passenger, stored as text (`sc:Text`).\\n6. **Age**: A float (`sc:Float`) indicating the age of the passenger in years.\\n7. **SibSp**: An integer (`sc:Integer`) denoting the number of siblings/spouses aboard the Titanic.\\n8. **Parch**: An integer (`sc:Integer`) for the number of parents/children aboard.\\n9. **Ticket**: The ticket number, represented as text (`sc:Text`).\\n10. **Fare**: A float (`sc:Float`) detailing the passenger's fare.\\n11. **Cabin**: The cabin number, stored as text (`sc:Text`).\\n12. **Embarked**: Textual data (`sc:Text`) encoding the port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\\n\\n#### Recommendations for Semantic Enhancement\\n\\n1. **PassengerId**: This field is currently internally significant but lacks external semantic utility. No external link suggestions.\\n   \\n2. **Survived and Pclass**: These categorical numerical fields could benefit from richer schema descriptors like `schema:EventAttendanceModeEnumeration` or `schema:Seat` for more explicit class data.\\n\\n3. **Name and Sex**: \\n   - **Name** could be enhanced with `schema:Person`.\\n   - **Sex** can potentially be linked to Wikidata concepts related to gender identity.\\n\\n4. **Age**: Already a float, could leverage schema fields like `schema:age` for integration into demographic studies.\\n\\n5. **SibSp and Parch**: These fields could be semantically enhanced using `schema:QuantitativeValue` to express familial relationships in more detail.\\n\\n6. **Ticket and Fare**: \\n   - **Ticket** numbers could link to transportation schema types though this field is typically not semantically enriched.\\n   - **Fare** can use `schema:PriceSpecification`.\\n\\n7. **Cabin**: Potentially enrich using `schema:LodgingUnitType` for a conceptual link to accommodations.\\n\\n8. **Embarked**: This field could be enhanced with `schema:Place` for links to known ports on Wikidata or schema.org locations.\\n\\n#### Overall Recommendations:\\nFields like `Name`, `Sex`, `Age`, and `Embarked` offer the most immediate opportunities for enrichment via established schemas and Wikidata links, aiding both data integration and enhancing contextual understanding in related datasets or applications.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n", "- created: 1738617456\n", "- model: gpt-4o-2024-08-06\n", "- object: chat.completion\n", "- service_tier: default\n", "- system_fingerprint: fp_4691090a87\n", "- usage: CompletionUsage(completion_tokens=641, prompt_tokens=2113, total_tokens=2754, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n", "\n", "</details>"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Is this getting us closer to our agentic contextualizer?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Yes, this is getting closer! The agent is now autonomously:\n", "1. Discovering the structure\n", "2. Making semantic connections\n", "3. Suggesting improvements\n", "\n", "But we're still missing key agentic behaviors from the Anthropic guide:\n", "\n", "1. **Dynamic Process Control**: The agent should decide its own exploration path rather than following our fixed 1-2-3-4 steps\n", "2. **Task Decomposition**: Currently it's doing everything in one big analysis rather than breaking it into manageable chunks\n", "3. **Tool Usage Flexibility**: It should be able to chain tool calls based on what it discovers\n", "\n", "Here's how we might evolve this to be more agentic:\n", "\n", "```python\n", "class SemanticAgent:\n", "    \"Agent that autonomously explores and enhances dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics,\n", "            self.fetch_wikidata,  # New: can look up semantic concepts\n", "            self.record_finding   # New: maintains state of discoveries\n", "        ])\n", "        self.findings = []  # Track what it learns\n", "    \n", "    def fetch_wikidata(self, concept:str) -> Dict:\n", "        \"Fetch information about a Wikidata concept\"\n", "        if not concept.startswith('Q'): return {'error':'Not a Wikidata ID'}\n", "        url = f\"https://www.wikidata.org/wiki/{concept}\"\n", "        try: return httpx.get(f\"{url}.json\").json()\n", "        except: return {'error':f'Failed to fetch {url}'}\n", "    \n", "    def record_finding(self, field:str, finding:str) -> bool:\n", "        \"Record a discovery about the dataset\"\n", "        self.findings.append({'field':field, 'finding':finding})\n", "        return True\n", "    \n", "    def explore(self, focus:str=None):\n", "        \"Start semantic exploration, optionally with a specific focus\"\n", "        prompt = f\"\"\"You are a semantic data explorer. Your goal is to understand and enhance this dataset's semantic structure.\n", "        \n", "        If you find something interesting, use record_finding to note it.\n", "        If you see a potential Wikidata concept, use fetch_wikidata to verify it.\n", "        \n", "        Current focus: {focus or 'Initial dataset exploration'}\n", "        \n", "        You decide what to explore next based on what you discover.\n", "        \"\"\"\n", "        return self.chat.toolloop(prompt)\n", "\n", "agent = SemanticAgent(data)\n", "```\n", "\n", "Key improvements:\n", "1. The agent maintains state (findings)\n", "2. It can focus on specific aspects but decides its own path\n", "3. It can verify semantic concepts by fetching them\n", "4. The prompt encourages autonomous exploration rather than following fixed steps\n", "\n", "Should we try this more agentic version and see how it explores the dataset?"]}, {"cell_type": "code", "metadata": {}, "source": ["class SemanticAgent:\n", "    \"Agent that autonomously explores and enhances dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.findings = []  # Track what it learns\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics,\n", "            self.fetch_wikidata,\n", "            self.record_finding\n", "        ])\n", "    \n", "    def list_fields(self) -> Dict[str,str]:\n", "        \"List all fields and their basic types\"\n", "        return {k: f.get('dataType',[]) for k,f in self.fields.items()}\n", "    \n", "    def check_field(self, name:str) -> Dict:\n", "        \"Get detailed information about a field\"\n", "        return self.fields.get(name, {})\n", "    \n", "    def suggest_semantics(self, field:str) -> Dict:\n", "        \"Suggest semantic enhancements for a field\"\n", "        desc = self.fields[field].get('description','')\n", "        types = self.fields[field].get('dataType',[])\n", "        return {\n", "            'current': {'desc': desc, 'types': types},\n", "            'field': field,\n", "        }\n", "    \n", "    def fetch_wikidata(self, concept:str) -> Dict:\n", "        \"Fetch information about a Wikidata concept\"\n", "        if not concept.startswith('Q'): return {'error':'Not a Wikidata ID'}\n", "        url = f\"https://www.wikidata.org/wiki/{concept}\"\n", "        try: return httpx.get(f\"{url}.json\").json()\n", "        except: return {'error':f'Failed to fetch {url}'}\n", "    \n", "    def record_finding(self, field:str, finding:str) -> bool:\n", "        \"Record a discovery about the dataset\"\n", "        self.findings.append({'field':field, 'finding':finding})\n", "        return True\n", "    \n", "    def explore(self, focus:str=None):\n", "        prompt = \"\"\"You are a semantic data explorer with access to these tools:\n", "        - list_fields: See what fields exist and their types\n", "        - check_field: Get details about specific fields\n", "        - suggest_semantics: Get enhancement suggestions\n", "        - fetch_wikidata: Look up semantic concepts\n", "        - record_finding: Save discoveries for later use\n", "\n", "        Exploration process:\n", "        1. First use list_fields to understand what's available\n", "        2. For each interesting field:\n", "           - Use check_field to get details\n", "           - Use suggest_semantics to find enhancement opportunities\n", "           - If semantic concepts are mentioned, verify with fetch_wikidata\n", "           - Record each discovery with record_finding\n", "        3. After each step, explain your thinking and next focus\n", "\n", "        Current focus: {focus or 'Initial dataset exploration'}\n", "        \n", "        Start exploring and share your discoveries as you go.\n", "        \"\"\"\n", "        return self.chat.toolloop(prompt, trace_func=print)  # Added trace_func to see tool usage\n", "\n", "agent = SemanticAgent(data)\n", "agent.explore()  # Let's see what it discovers!"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["ChatCompletion(id='chatcmpl-AwyHWz9y6o73nbbFl0BIOsnfYG3Jw', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_BS21zhzMgw1V8r6yR1U0BKN7', function=Function(arguments='{}', name='list_fields'), type='function')]))], created=1738618110, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 352; Out: 11; Total: 363)\n", "ChatCompletion(id='chatcmpl-AwyHXLLfIdp2vBIrxOTgfiJqJNEiC', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='The dataset contains the following fields along with their types:\\n\\n- **PassengerId**: Text\\n- **Survived**: Integer\\n- **Pclass**: Integer\\n- **Name**: Text\\n- **Sex**: Text\\n- **Age**: Float\\n- **SibSp**: Integer\\n- **Parch**: Integer\\n- **Ticket**: Text\\n- **Fare**: Float\\n- **Cabin**: Text\\n- **Embarked**: Text\\n\\n### Thinking:\\nEach field represents distinct information about passengers, possibly from a Titanic dataset. The field \"Survived\" immediately stands out because it might be crucial for any analysis related to survival rates. Fields like \"Age,\" \"Sex,\" \"Pclass,\" and \"Fare\" might also hold significant insights about passenger demographics and socio-economic factors.\\n\\n### Next Steps:\\n1. **Check field details** for \"Survived,\" \"Pclass,\" \"Age,\" and \"Fare.\"\\n2. **Suggest semantics** for the same fields since these are likely to hold meaningful data.\\n3. Verify and record semantic concepts using `fetch_wikidata` and `record_finding`.\\n\\nLet\\'s start by exploring the details and semantics for each of these fields.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_3BJsyIXP1FzdmbUqXgU32UMV', function=Function(arguments='{\"name\": \"Survived\"}', name='check_field'), type='function'), ChatCompletionMessageToolCall(id='call_6t9gpnGzTdWAAyzSOFFT8Ptx', function=Function(arguments='{\"field\": \"Survived\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_Ago8OQY1gGkSZvLiVQhu1XSE', function=Function(arguments='{\"name\": \"Pclass\"}', name='check_field'), type='function'), ChatCompletionMessageToolCall(id='call_BRuWzy79R5PTp5KJz9VqQyzH', function=Function(arguments='{\"field\": \"Pclass\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_2voNUANd61k3j5lhQBFzDu6L', function=Function(arguments='{\"name\": \"Age\"}', name='check_field'), type='function'), ChatCompletionMessageToolCall(id='call_9NiO0SpVHv0iWjOYRUuAFOmU', function=Function(arguments='{\"field\": \"Age\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_WjZJOwnlW2yA5Z3tuBf32nWV', function=Function(arguments='{\"name\": \"Fare\"}', name='check_field'), type='function'), ChatCompletionMessageToolCall(id='call_3bupvmX4zTVuY3VtlA7TYrGB', function=Function(arguments='{\"field\": \"Fare\"}', name='suggest_semantics'), type='function')]))], created=1738618111, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 475; Out: 399; Total: 874)\n", "ChatCompletion(id='chatcmpl-AwyHh4Jf77X0UsnFfmGiuu6yYYw6S', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='### Field Details and Suggested Semantics:\\n\\n1. **Survived**:\\n   - **Details**: Indicates whether a passenger survived (0 = No, 1 = Yes). It\\'s an integer field.\\n   - **Current Semantics**: Weather Survived or not: 0 = No, 1 = Yes.\\n\\n2. **Pclass**:\\n   - **Details**: Represents the ticket class (1 = 1st, 2 = 2nd, 3 = 3rd). It\\'s an integer field.\\n   - **Current Semantics**: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd.\\n\\n3. **Age**:\\n   - **Details**: Specifies the age of the passenger in years. It\\'s a float field.\\n   - **Current Semantics**: Age in Years.\\n\\n4. **Fare**:\\n   - **Details**: The fare paid by the passenger. It\\'s a float field.\\n   - **Current Semantics**: Passenger fare.\\n\\n### Analysis and Next Steps:\\n- The fields \"Survived\" and \"Pclass\" are categorical and can significantly influence survival-related analyses.\\n- \"Age\" is a key demographic variable potentially impacting survival.\\n- \"Fare\" might reflect socio-economic status and its effect on survival.\\n\\n### Next Focus:\\n1. For the fields \"Survived\" and \"Pclass,\" verify semantic concepts using `fetch_wikidata`.\\n2. Record discoveries using `record_finding`.\\n\\nI\\'ll now verify these semantic concepts.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_hmjwxfghCeWE1EbdypZnq9cK', function=Function(arguments='{\"concept\": \"Survived\"}', name='fetch_wikidata'), type='function'), ChatCompletionMessageToolCall(id='call_7YlUIy8Ky9TgWB7LVd2ulWUj', function=Function(arguments='{\"concept\": \"Pclass\"}', name='fetch_wikidata'), type='function'), ChatCompletionMessageToolCall(id='call_jEpDq5bVcuXTb3nH9LQcTlFj', function=Function(arguments='{\"field\": \"Survived\", \"finding\": \"Weather Survived or not: 0 = No, 1 = Yes\"}', name='record_finding'), type='function'), ChatCompletionMessageToolCall(id='call_LuxTF6cw1iL1s50BkH2qfrvH', function=Function(arguments='{\"field\": \"Pclass\", \"finding\": \"Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\"}', name='record_finding'), type='function'), ChatCompletionMessageToolCall(id='call_2JM7LfxLsN08pia1w3EAof5Q', function=Function(arguments='{\"field\": \"Age\", \"finding\": \"Age in Years\"}', name='record_finding'), type='function'), ChatCompletionMessageToolCall(id='call_tIvofHovwdC2akzxwRg7pEh9', function=Function(arguments='{\"field\": \"Fare\", \"finding\": \"Passenger fare\"}', name='record_finding'), type='function')]))], created=1738618121, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 1372; Out: 491; Total: 1863)\n", "ChatCompletion(id='chatcmpl-AwyHsNFucFcvqQxyF0Xyxg57hLQ8A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='### Findings:\\n\\n- **Semantic Concepts Verification**:\\n  - The concept \"Survived\" could not be directly linked to a Wikidata ID. It might be a dataset-specific term.\\n  - Similarly, \"Pclass,\" as defined here, might not match a global concept with a Wikidata ID, given its specific context within the dataset.\\n\\n- **Recorded Discoveries**:\\n  - Successfully recorded the finding for \"Survived\" as \"Weather Survived or not: 0 = No, 1 = Yes.\"\\n  - Successfully recorded for \"Pclass\" as \"Ticket class: 1 = 1st, 2nd, 3rd.\"\\n  - Successfully recorded for \"Age\" as \"Age in Years.\"\\n  - Successfully recorded for \"Fare\" as \"Passenger fare.\"\\n\\n### Conclusion:\\nThese fields are specifically meaningful within the context of the Titanic dataset and reflect significant demographic and outcome-related insights. The next steps could focus on other fields such as \"Sex\" and \"Embarked\" to complete the semantic exploration of this dataset. If you wish to explore these or other fields further, please let me know!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738618132, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 1910; Out: 233; Total: 2143)\n"]}, {"data": {"text/plain": ["ChatCompletion(id='chatcmpl-AwyHsNFucFcvqQxyF0Xyxg57hLQ8A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='### Findings:\\n\\n- **Semantic Concepts Verification**:\\n  - The concept \"Survived\" could not be directly linked to a Wikidata ID. It might be a dataset-specific term.\\n  - Similarly, \"Pclass,\" as defined here, might not match a global concept with a Wikidata ID, given its specific context within the dataset.\\n\\n- **Recorded Discoveries**:\\n  - Successfully recorded the finding for \"Survived\" as \"Weather Survived or not: 0 = No, 1 = Yes.\"\\n  - Successfully recorded for \"Pclass\" as \"Ticket class: 1 = 1st, 2nd, 3rd.\"\\n  - Successfully recorded for \"Age\" as \"Age in Years.\"\\n  - Successfully recorded for \"Fare\" as \"Passenger fare.\"\\n\\n### Conclusion:\\nThese fields are specifically meaningful within the context of the Titanic dataset and reflect significant demographic and outcome-related insights. The next steps could focus on other fields such as \"Sex\" and \"Embarked\" to complete the semantic exploration of this dataset. If you wish to explore these or other fields further, please let me know!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738618132, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 1910; Out: 233; Total: 2143)"], "text/markdown": ["### Findings:\n", "\n", "- **Semantic Concepts Verification**:\n", "  - The concept \"Survived\" could not be directly linked to a Wikidata ID. It might be a dataset-specific term.\n", "  - Similarly, \"Pclass,\" as defined here, might not match a global concept with a Wikidata ID, given its specific context within the dataset.\n", "\n", "- **Recorded Discoveries**:\n", "  - Successfully recorded the finding for \"Survived\" as \"Weather Survived or not: 0 = No, 1 = Yes.\"\n", "  - Successfully recorded for \"Pclass\" as \"Ticket class: 1 = 1st, 2nd, 3rd.\"\n", "  - Successfully recorded for \"Age\" as \"Age in Years.\"\n", "  - Successfully recorded for \"Fare\" as \"Passenger fare.\"\n", "\n", "### Conclusion:\n", "These fields are specifically meaningful within the context of the Titanic dataset and reflect significant demographic and outcome-related insights. The next steps could focus on other fields such as \"Sex\" and \"Embarked\" to complete the semantic exploration of this dataset. If you wish to explore these or other fields further, please let me know!\n", "\n", "<details>\n", "\n", "- id: chatcmpl-AwyHsNFucFcvqQxyF0Xyxg57hLQ8A\n", "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='### Findings:\\n\\n- **Semantic Concepts Verification**:\\n  - The concept \"Survived\" could not be directly linked to a Wikidata ID. It might be a dataset-specific term.\\n  - Similarly, \"Pclass,\" as defined here, might not match a global concept with a Wikidata ID, given its specific context within the dataset.\\n\\n- **Recorded Discoveries**:\\n  - Successfully recorded the finding for \"Survived\" as \"Weather Survived or not: 0 = No, 1 = Yes.\"\\n  - Successfully recorded for \"Pclass\" as \"Ticket class: 1 = 1st, 2nd, 3rd.\"\\n  - Successfully recorded for \"Age\" as \"Age in Years.\"\\n  - Successfully recorded for \"Fare\" as \"Passenger fare.\"\\n\\n### Conclusion:\\nThese fields are specifically meaningful within the context of the Titanic dataset and reflect significant demographic and outcome-related insights. The next steps could focus on other fields such as \"Sex\" and \"Embarked\" to complete the semantic exploration of this dataset. If you wish to explore these or other fields further, please let me know!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n", "- created: 1738618132\n", "- model: gpt-4o-2024-08-06\n", "- object: chat.completion\n", "- service_tier: default\n", "- system_fingerprint: fp_4691090a87\n", "- usage: CompletionUsage(completion_tokens=233, prompt_tokens=1910, total_tokens=2143, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1792))\n", "\n", "</details>"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Is this what we expected from the prototype?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Yes and no. The agent is showing some promising behaviors but also revealing areas that need improvement:\n", "\n", "Good aspects:\n", "1. It's autonomously exploring the data structure\n", "2. It's making its own decisions about what to investigate next\n", "3. It's maintaining state through the findings list\n", "4. It's trying to verify semantic concepts\n", "\n", "Problems:\n", "1. It's not making good use of Wikidata - it tried to look up \"Survived\" and \"Pclass\" directly instead of finding relevant concepts (like `wd:Q48277` for gender)\n", "2. It's not really enhancing the semantic structure - just describing what's there\n", "3. The tool usage is somewhat mechanical rather than truly adaptive\n", "4. It's not leveraging the JSON-LD context we discovered earlier\n", "\n", "Let's improve it to be more semantically aware:\n", "\n", "```python\n", "class SemanticAgent:\n", "    \"Agent that autonomously explores and enhances dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.findings = []\n", "        self.semantic_context = self.data.get('@context',{})\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics,\n", "            self.fetch_wikidata,\n", "            self.record_finding,\n", "            self.get_namespace  # New: understand semantic context\n", "        ])\n", "    \n", "    def get_namespace(self, prefix:str) -> str:\n", "        \"Get full URI for a namespace prefix\"\n", "        return self.semantic_context.get(prefix,'')\n", "    \n", "    def fetch_wikidata(self, concept:str) -> Dict:\n", "        \"Fetch information about a Wikidata concept\"\n", "        # Now handles both Q ids and semantic concepts\n", "        if concept.startswith('wd:'): \n", "            concept = concept.split(':')[1]\n", "        elif not concept.startswith('Q'): \n", "            return {'error':'Not a Wikidata ID or CURIE'}\n", "        url = f\"https://www.wikidata.org/wiki/{concept}\"\n", "        try: return httpx.get(f\"{url}.json\").json()\n", "        except: return {'error':f'Failed to fetch {url}'}\n", "\n", "    def explore(self, focus:str=None):\n", "        prompt = \"\"\"You are a semantic data explorer specializing in Linked Data.\n", "        Available namespaces include schema.org (sc:), Wikidata (wd:), and MLCommons Croissant (cr:).\n", "        \n", "        For each field:\n", "        1. Consider what semantic concepts it represents\n", "        2. Look for relevant Wikidata concepts (e.g. wd:Q48277 for gender)\n", "        3. Suggest richer schema.org types where appropriate\n", "        4. Record your findings about potential semantic enhancements\n", "        \n", "        Current focus: {focus or 'Initial dataset exploration'}\n", "        \"\"\"\n", "        return self.chat.toolloop(prompt)\n", "```\n", "\n", "The key differences:\n", "1. Added explicit knowledge of semantic namespaces\n", "2. Improved Wikidata lookup to handle CURIEs\n", "3. More semantically-focused prompt\n", "4. Better guidance on finding relevant concepts\n", "\n", "Want to try this more semantically-aware version?"]}, {"cell_type": "code", "metadata": {}, "source": ["class SemanticAgent:\n", "    \"Agent that autonomously explores and enhances dataset semantics\"\n", "    def __init__(self, data, model=models[2]):\n", "        self.data = data\n", "        self.fields = {f['name']:f for rs in data.get('recordSet',[]) \n", "                      for f in rs.get('field',[])}\n", "        self.findings = []\n", "        self.semantic_context = self.data.get('@context',{})\n", "        self.chat = Chat(model, tools=[\n", "            self.list_fields,\n", "            self.check_field,\n", "            self.suggest_semantics\n", "        ])\n", "    \n", "    def list_fields(self):\n", "        \"List all fields and their basic types\"\n", "        return {k: f.get('dataType',[]) for k,f in self.fields.items()}\n", "    \n", "    def check_field(self, field_name:str):\n", "        \"Get detailed information about a field\"\n", "        return self.fields.get(field_name, {})\n", "    \n", "    def suggest_semantics(self, field_name:str):\n", "        \"Suggest semantic enhancements for a field\"\n", "        field = self.fields.get(field_name,{})\n", "        return {\n", "            'description': field.get('description',''),\n", "            'types': field.get('dataType',[]),\n", "            'context': self.semantic_context\n", "        }\n", "    \n", "    def explore(self):\n", "        \"Start semantic exploration\"\n", "        prompt = \"\"\"You are a semantic data explorer. Your goal is to:\n", "        1. Examine the available fields\n", "        2. For each field, suggest semantic enhancements using:\n", "           - schema.org types (sc:)\n", "           - wikidata concepts (wd:)\n", "           - MLCommons Croissant vocab (cr:)\n", "        \n", "        Start by listing the fields and then examine each one.\n", "        \"\"\"\n", "        return self.chat.toolloop(prompt, trace_func=print)\n", "\n", "agent = SemanticAgent(data)\n", "agent.explore()"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["ChatCompletion(id='chatcmpl-AwyYkr4GsiILh5r9CESZPkdU6zgpQ', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mKMwuhmyoYAXisL7GbZCeZiW', function=Function(arguments='{}', name='list_fields'), type='function')]))], created=1738619178, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 157; Out: 11; Total: 168)\n", "ChatCompletion(id='chatcmpl-AwyYlQ6oy8PtvgkK9QBr4s0Wmb4AP', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_jt1dPkxGMlr6kisno9m7VszZ', function=Function(arguments='{\"field_name\": \"PassengerId\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_gnq0dXmuOR7OUI7LQLfhLwF4', function=Function(arguments='{\"field_name\": \"Survived\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_EnSz5uL7tIRokUHq9t7R083l', function=Function(arguments='{\"field_name\": \"Pclass\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_gPNvPthlfawnmIxeLSr95jwe', function=Function(arguments='{\"field_name\": \"Name\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_SlR0HSoYJ8hfPjANP5TEZgqS', function=Function(arguments='{\"field_name\": \"Sex\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_YOsk38LtBIn0FaPsmX0EOOvc', function=Function(arguments='{\"field_name\": \"Age\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_0AsQV1vxMdeSMSQuAFCdkfac', function=Function(arguments='{\"field_name\": \"SibSp\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_ECBoV7onu5YtbyX6CocbnJ6v', function=Function(arguments='{\"field_name\": \"Parch\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_Q7lANP2euM7lhQ6gts1rnNkZ', function=Function(arguments='{\"field_name\": \"Ticket\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_DJGHKUFXXB31CvziJjt9qPhk', function=Function(arguments='{\"field_name\": \"Fare\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_F5Dn4HnmgrelTdOBmzoFUgeh', function=Function(arguments='{\"field_name\": \"Cabin\"}', name='suggest_semantics'), type='function'), ChatCompletionMessageToolCall(id='call_2QXAkc9IlN3CkIega1g6JPVR', function=Function(arguments='{\"field_name\": \"Embarked\"}', name='suggest_semantics'), type='function')]))], created=1738619179, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 280; Out: 230; Total: 510)\n", "ChatCompletion(id='chatcmpl-AwyYwu5LuYtVNKNYnwgh7ZxO8Ablx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's the semantic enhancement analysis of the available fields from the dataset:\\n\\n1. **PassengerId**\\n   - Description: Passenger ID\\n   - Suggested Types: `sc:Text`\\n\\n2. **Survived**\\n   - Description: Whether Survived or not: 0 = No, 1 = Yes\\n   - Suggested Types: `sc:Integer`\\n\\n3. **Pclass**\\n   - Description: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\\n   - Suggested Types: `sc:Integer`\\n\\n4. **Name**\\n   - Description: Name of the Passenger\\n   - Suggested Types: `sc:Text`\\n\\n5. **Sex**\\n   - Description: Gender\\n   - Suggested Types: `sc:Text`\\n\\n6. **Age**\\n   - Description: Age in Years\\n   - Suggested Types: `sc:Float`\\n\\n7. **SibSp**\\n   - Description: No. of siblings/spouses aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n8. **Parch**\\n   - Description: No. of parents/children aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n9. **Ticket**\\n   - Description: Ticket number\\n   - Suggested Types: `sc:Text`\\n\\n10. **Fare**\\n    - Description: Passenger fare\\n    - Suggested Types: `sc:Float`\\n\\n11. **Cabin**\\n    - Description: Cabin number\\n    - Suggested Types: `sc:Text`\\n\\n12. **Embarked**\\n    - Description: Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\\n    - Suggested Types: `sc:Text`\\n\\nThese semantic suggestions can be mapped to more specific vocabularies if necessary, using schema.org types (`sc:`), wikidata concepts (`wd:`), and MLCommons Croissant vocabularies (`cr:`).\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738619190, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 5111; Out: 398; Total: 5509)\n"]}, {"data": {"text/plain": ["ChatCompletion(id='chatcmpl-AwyYwu5LuYtVNKNYnwgh7ZxO8Ablx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's the semantic enhancement analysis of the available fields from the dataset:\\n\\n1. **PassengerId**\\n   - Description: Passenger ID\\n   - Suggested Types: `sc:Text`\\n\\n2. **Survived**\\n   - Description: Whether Survived or not: 0 = No, 1 = Yes\\n   - Suggested Types: `sc:Integer`\\n\\n3. **Pclass**\\n   - Description: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\\n   - Suggested Types: `sc:Integer`\\n\\n4. **Name**\\n   - Description: Name of the Passenger\\n   - Suggested Types: `sc:Text`\\n\\n5. **Sex**\\n   - Description: Gender\\n   - Suggested Types: `sc:Text`\\n\\n6. **Age**\\n   - Description: Age in Years\\n   - Suggested Types: `sc:Float`\\n\\n7. **SibSp**\\n   - Description: No. of siblings/spouses aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n8. **Parch**\\n   - Description: No. of parents/children aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n9. **Ticket**\\n   - Description: Ticket number\\n   - Suggested Types: `sc:Text`\\n\\n10. **Fare**\\n    - Description: Passenger fare\\n    - Suggested Types: `sc:Float`\\n\\n11. **Cabin**\\n    - Description: Cabin number\\n    - Suggested Types: `sc:Text`\\n\\n12. **Embarked**\\n    - Description: Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\\n    - Suggested Types: `sc:Text`\\n\\nThese semantic suggestions can be mapped to more specific vocabularies if necessary, using schema.org types (`sc:`), wikidata concepts (`wd:`), and MLCommons Croissant vocabularies (`cr:`).\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738619190, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 5111; Out: 398; Total: 5509)"], "text/markdown": ["Here's the semantic enhancement analysis of the available fields from the dataset:\n", "\n", "1. **PassengerId**\n", "   - Description: Passenger ID\n", "   - Suggested Types: `sc:Text`\n", "\n", "2. **Survived**\n", "   - Description: Whether Survived or not: 0 = No, 1 = Yes\n", "   - Suggested Types: `sc:Integer`\n", "\n", "3. **Pclass**\n", "   - Description: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\n", "   - Suggested Types: `sc:Integer`\n", "\n", "4. **Name**\n", "   - Description: Name of the Passenger\n", "   - Suggested Types: `sc:Text`\n", "\n", "5. **Sex**\n", "   - Description: Gender\n", "   - Suggested Types: `sc:Text`\n", "\n", "6. **Age**\n", "   - Description: Age in Years\n", "   - Suggested Types: `sc:Float`\n", "\n", "7. **SibSp**\n", "   - Description: No. of siblings/spouses aboard the Titanic\n", "   - Suggested Types: `sc:Integer`\n", "\n", "8. **Parch**\n", "   - Description: No. of parents/children aboard the Titanic\n", "   - Suggested Types: `sc:Integer`\n", "\n", "9. **Ticket**\n", "   - Description: Ticket number\n", "   - Suggested Types: `sc:Text`\n", "\n", "10. **Fare**\n", "    - Description: Passenger fare\n", "    - Suggested Types: `sc:Float`\n", "\n", "11. **Cabin**\n", "    - Description: Cabin number\n", "    - Suggested Types: `sc:Text`\n", "\n", "12. **Embarked**\n", "    - Description: Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\n", "    - Suggested Types: `sc:Text`\n", "\n", "These semantic suggestions can be mapped to more specific vocabularies if necessary, using schema.org types (`sc:`), wikidata concepts (`wd:`), and MLCommons Croissant vocabularies (`cr:`).\n", "\n", "<details>\n", "\n", "- id: chatcmpl-AwyYwu5LuYtVNKNYnwgh7ZxO8Ablx\n", "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's the semantic enhancement analysis of the available fields from the dataset:\\n\\n1. **PassengerId**\\n   - Description: Passenger ID\\n   - Suggested Types: `sc:Text`\\n\\n2. **Survived**\\n   - Description: Whether Survived or not: 0 = No, 1 = Yes\\n   - Suggested Types: `sc:Integer`\\n\\n3. **Pclass**\\n   - Description: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\\n   - Suggested Types: `sc:Integer`\\n\\n4. **Name**\\n   - Description: Name of the Passenger\\n   - Suggested Types: `sc:Text`\\n\\n5. **Sex**\\n   - Description: Gender\\n   - Suggested Types: `sc:Text`\\n\\n6. **Age**\\n   - Description: Age in Years\\n   - Suggested Types: `sc:Float`\\n\\n7. **SibSp**\\n   - Description: No. of siblings/spouses aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n8. **Parch**\\n   - Description: No. of parents/children aboard the Titanic\\n   - Suggested Types: `sc:Integer`\\n\\n9. **Ticket**\\n   - Description: Ticket number\\n   - Suggested Types: `sc:Text`\\n\\n10. **Fare**\\n    - Description: Passenger fare\\n    - Suggested Types: `sc:Float`\\n\\n11. **Cabin**\\n    - Description: Cabin number\\n    - Suggested Types: `sc:Text`\\n\\n12. **Embarked**\\n    - Description: Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\\n    - Suggested Types: `sc:Text`\\n\\nThese semantic suggestions can be mapped to more specific vocabularies if necessary, using schema.org types (`sc:`), wikidata concepts (`wd:`), and MLCommons Croissant vocabularies (`cr:`).\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n", "- created: 1738619190\n", "- model: gpt-4o-2024-08-06\n", "- object: chat.completion\n", "- service_tier: default\n", "- system_fingerprint: fp_50cad350e4\n", "- usage: CompletionUsage(completion_tokens=398, prompt_tokens=5111, total_tokens=5509, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n", "\n", "</details>"]}, "metadata": {}, "output_type": "execute_result", "execution_count": null}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "What do we think of this output?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "This output is still not achieving what we want. The problems are:\n", "\n", "1. It's just listing the basic types (`sc:Text`, `sc:Integer`, etc.) without adding semantic value\n", "2. It's not suggesting any Wikidata concepts (like `wd:Q48277` for gender)\n", "3. It's not using the context it has access to\n", "4. It's not really being \"agentic\" - it's just following a mechanical process\n", "\n", "What we really want is an agent that:\n", "1. Understands semantic relationships\n", "2. Makes intelligent suggestions for enrichment\n", "3. Can explain its reasoning\n", "4. Builds up knowledge as it explores\n", "\n", "Should we try a different approach, perhaps focusing first on a single field (like \"Sex\" or \"Embarked\") and having the agent really explore the semantic possibilities before moving on? This might help us understand what additional capabilities the agent needs.\n", "\n", "Would you like to try that more focused approach?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Ok. Summarize what we are trying to do in this solveit notebook."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "We're trying to build an agentic system that can:\n", "\n", "1. Autonomously explore and understand dataset semantics\n", "2. Leverage semantic web standards (RDF, JSON-LD, Wikidata concepts)\n", "3. Suggest meaningful semantic enhancements to make datasets more FAIR (Findable, Accessible, Interoperable, Reusable)\n", "\n", "Using the Titanic dataset as our example, we're following Jeremy Howard's iterative approach:\n", "1. Start simple\n", "2. Test and observe\n", "3. Identify what's missing\n", "4. Improve incrementally\n", "\n", "So far we've learned that:\n", "- Basic metadata exploration works\n", "- But true semantic understanding requires more sophisticated reasoning\n", "- The agent needs better tools for semantic concept discovery\n", "- We should probably focus on one aspect (like a single field) first\n", "\n", "Would you like to focus on one of these aspects to develop further?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Reflect on the solveit process, of using a LLM to assist in building code for agentic systems, evaluate and suggest refinements to prompting to enhance agent output. Also on the fact the solveit LLM was writing functions to expore the data set and came to interesting conclusions which suggests a possible path to building a more general agentic explorer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "The solveit process revealed several interesting insights about building agentic systems:\n", "\n", "1. **Iterative Development with LLM Assistance**:\n", "   - The LLM was able to suggest code improvements\n", "   - It identified gaps in semantic understanding\n", "   - It adapted its suggestions based on observed results\n", "   - This suggests LLMs can be effective \"pair programmers\" for agent development\n", "\n", "2. **Emergent Exploration Pattern**:\n", "   - The LLM naturally moved from simple listing to deeper semantic analysis\n", "   - It suggested improvements to its own code\n", "   - It identified when approaches weren't working\n", "   - This hints at meta-learning capabilities we could leverage\n", "\n", "3. **Areas for Improvement**:\n", "   - The prompting could be more focused on reasoning about semantic relationships\n", "   - We could better leverage the LLM's ability to understand context\n", "   - Tool definitions could be more flexible and self-describing\n", "   - The agent could build up knowledge more systematically\n", "\n", "Would you like to explore any of these aspects further, particularly how we might improve the prompting to create more sophisticated semantic exploration?"]}], "metadata": {"kernelspec": {"display_name": "python3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}